{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils import shuffle, resample\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from keras.utils import load_img, img_to_array, to_categorical\n",
    "from keras.models import model_from_json, Model\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "    'present_output': 'binary_crossentropy',\n",
    "    'grading_output': 'categorical_crossentropy'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('path/to/DDR/DR_grading/labels.csv')\n",
    "df = df[df['label'] != 5]\n",
    "dataset_path = 'path/to/DDR/DR_grading/all'\n",
    "\n",
    "# Define constants\n",
    "IMAGE_SIZE = (256, 256)  # Example size, adjust to your models\n",
    "NUM_CLASSES = 5  # Grading levels 0-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target sample sizes for each label\n",
    "target_sizes = {\n",
    "    0: 500,\n",
    "    1: 500,\n",
    "    2: 500,\n",
    "    3: 500,\n",
    "    4: 500\n",
    "}\n",
    "\n",
    "# Initialize a list to hold the sliced DataFrames\n",
    "sliced_dfs = []\n",
    "\n",
    "# Slice the DataFrame for each label\n",
    "for label, size in target_sizes.items():\n",
    "    class_df = df[df['label'] == label]\n",
    "    if len(class_df) >= size:\n",
    "        # Undersample if the class size is greater than or equal to the target size\n",
    "        sliced_df = class_df.sample(size, random_state=42)\n",
    "    else:\n",
    "        # Oversample if the class size is smaller than the target size\n",
    "        sliced_df = resample(class_df, replace=True, n_samples=size, random_state=42)\n",
    "    sliced_dfs.append(sliced_df)\n",
    "\n",
    "# Combine all sliced DataFrames\n",
    "test_df = pd.concat(sliced_dfs)\n",
    "\n",
    "# Shuffle the final dataset\n",
    "test_df = shuffle(test_df, random_state=42)\n",
    "\n",
    "test_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet import preprocess_input as resenet50_preprocess_input\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models_TL/ResNet50_pretrained_enhanced.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights('models_TL/ResNet50_pretrained_enhanced.weights.h5')\n",
    "model.compile(optimizer='adam', loss=losses, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_img(img):\n",
    "    # Step 1: Apply median filter with a 3x3 kernel\n",
    "    img = cv2.medianBlur(img.astype(np.uint8), ksize=3)\n",
    "\n",
    "    # Step 2: Convert to LAB color space\n",
    "    lab_img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab_img)\n",
    "\n",
    "    # Step 3: Apply CLAHE on the Luminosity (L) channel with 8x8 tile grid\n",
    "    clahe = cv2.createCLAHE(clipLimit=6.0, tileGridSize=(8, 8))\n",
    "    l = clahe.apply(l)\n",
    "\n",
    "    # Step 4: Merge CLAHE enhanced L with original A and B channels\n",
    "    lab_img = cv2.merge((l, a, b))\n",
    "\n",
    "    # Step 5: Convert back to RGB color space\n",
    "    enhanced_img = cv2.cvtColor(lab_img, cv2.COLOR_LAB2RGB)\n",
    "    \n",
    "    return enhanced_img\n",
    "\n",
    "# Prepare dataset\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing rows\"):\n",
    "    image_path = row['image']\n",
    "    label = row['label']\n",
    "    \n",
    "    image_path = os.path.join(dataset_path, image_path)\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    x = img_to_array(img)\n",
    "    x = enhance_img(x)\n",
    "    x = resenet50_preprocess_input(x)\n",
    "    X.append(x)\n",
    "    y.append(label)\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X.shape, y.shape\n",
    "\n",
    "y_present = (y > 0).astype(int)  # Binary: 0 (no disease), 1 (disease present)\n",
    "y_grades = np.where(y_present == 1, y, 0)  # Multiclass: 1-4 if disease present, 0 otherwise\n",
    "y_grades = to_categorical(y_grades, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X)\n",
    "present_predictions = (predictions[0] > 0.5).astype(int)  # Binary output\n",
    "grade_predictions = np.argmax(predictions[1], axis=1)  # Multiclass output\n",
    "true_grades = np.argmax(y_grades, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(true_grades, grade_predictions)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(5), yticklabels=range(5))\n",
    "plt.xlabel(\"Predicted Grades\")\n",
    "plt.ylabel(\"True Grades\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_grades, grade_predictions)\n",
    "precision = precision_score(true_grades, grade_predictions, average='weighted')\n",
    "recall = recall_score(true_grades, grade_predictions, average='weighted')\n",
    "classification_rep = classification_report(true_grades, grade_predictions, target_names=[f'Grade {i}' for i in range(5)])\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted Cohen's kappa\n",
    "kappa = cohen_kappa_score(true_grades, grade_predictions, weights='linear')\n",
    "print(f\"Cohen's Kappa (weighted): {kappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted Cohen's kappa\n",
    "kappa = cohen_kappa_score(true_grades, grade_predictions, weights='quadratic')\n",
    "print(f\"Cohen's Kappa (weighted): {kappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict presence (binary)\n",
    "binary_predictions = (predictions[0] > 0.5).astype(int)  # Threshold at 0.5\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_present, binary_predictions)\n",
    "precision = precision_score(y_present, binary_predictions, average='binary')\n",
    "recall = recall_score(y_present, binary_predictions, average='binary')\n",
    "classification_rep = classification_report(y_present, binary_predictions, target_names=['No Disease', 'Disease'])\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models_features/CNN_all_features.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights('models_features/CNN_all_features.weights.h5')\n",
    "model.compile(optimizer='adam', loss=losses, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EX\n",
    "with open('models_segmentation/EX.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "model_hard_exudates = model_from_json(model_json)\n",
    "model_hard_exudates.load_weights('models_segmentation/EX.weights.h5')\n",
    "model_hard_exudates.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# SE\n",
    "with open('models_segmentation/SE.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "model_soft_exudates = model_from_json(model_json)\n",
    "model_soft_exudates.load_weights('models_segmentation/SE.weights.h5')\n",
    "model_soft_exudates.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# HE\n",
    "with open('models_segmentation/HE.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "model_haemorrhages = model_from_json(model_json)\n",
    "model_haemorrhages.load_weights('models_segmentation/HE.weights.h5')\n",
    "model_haemorrhages.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# MA\n",
    "with open('models_segmentation/MA.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "model_microaneurysms = model_from_json(model_json)\n",
    "model_microaneurysms.load_weights('models_segmentation/MA.weights.h5')\n",
    "model_microaneurysms.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_img(img):\n",
    "    # Step 1: Apply median filter with a 3x3 kernel\n",
    "    img = cv2.medianBlur(img.astype(np.uint8), ksize=3)\n",
    "\n",
    "    # Step 2: Convert to LAB color space\n",
    "    lab_img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab_img)\n",
    "\n",
    "    # Step 3: Apply CLAHE on the Luminosity (L) channel with 8x8 tile grid\n",
    "    clahe = cv2.createCLAHE(clipLimit=6.0, tileGridSize=(8, 8))\n",
    "    l = clahe.apply(l)\n",
    "\n",
    "    # Step 4: Merge CLAHE enhanced L with original A and B channels\n",
    "    lab_img = cv2.merge((l, a, b))\n",
    "\n",
    "    # Step 5: Convert back to RGB color space\n",
    "    enhanced_img = cv2.cvtColor(lab_img, cv2.COLOR_LAB2RGB)\n",
    "    \n",
    "    return enhanced_img\n",
    "\n",
    "def get_mask(path, target_size):     \n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.resize(image, target_size)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert image to HSV (Hue, Saturation, Value) color space\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the red color range for masking\n",
    "    lower_red = np.array([0, 100, 50])\n",
    "    upper_red = np.array([12, 250, 250])\n",
    "    mask1 = cv2.inRange(hsv_image, lower_red, upper_red)\n",
    "\n",
    "    # lower_red2 = np.array([170, 120, 70])\n",
    "    lower_red2 = np.array([10, 60, 70])\n",
    "    upper_red2 = np.array([180, 255, 255])\n",
    "    mask2 = cv2.inRange(hsv_image, lower_red2, upper_red2)\n",
    "    mask2 = cv2.bitwise_not(mask2)\n",
    "\n",
    "    # Combine masks for red\n",
    "    red_mask = mask1 + mask2\n",
    "    \n",
    "    # overlay the mask on the original image\n",
    "    red_mask_3ch = cv2.cvtColor(red_mask, cv2.COLOR_GRAY2BGR)\n",
    "    mask = cv2.addWeighted(image_rgb, 0.7, red_mask_3ch, 0.3, 0)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Define function to preprocess images\n",
    "def preprocess_image_EX(image_path):\n",
    "    img = load_img(image_path, target_size=IMAGE_SIZE)\n",
    "    img = img_to_array(img)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def preprocess_image_SE(image_path):\n",
    "    img = load_img(image_path, target_size=IMAGE_SIZE)\n",
    "    img = img_to_array(img)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def preprocess_image_HE(image_path):\n",
    "    img = load_img(image_path, target_size=IMAGE_SIZE)\n",
    "    img = img_to_array(img)\n",
    "    img = enhance_img(img)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def preprocess_image_MA(image_path):\n",
    "    img = img = get_mask(image_path, target_size=IMAGE_SIZE)\n",
    "    img = img_to_array(img)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def generate_feature_maps(image_path, size=(128, 128)):\n",
    "    # Apply the specific preprocessing method for each model\n",
    "    img_hard = preprocess_image_EX(image_path)\n",
    "    img_soft = preprocess_image_SE(image_path)\n",
    "    img_haem = preprocess_image_HE(image_path)\n",
    "    img_micro = preprocess_image_MA(image_path)\n",
    "\n",
    "    # Add batch dimensions for predictions\n",
    "    img_hard = np.expand_dims(img_hard, axis=0)\n",
    "    img_soft = np.expand_dims(img_soft, axis=0)\n",
    "    img_haem = np.expand_dims(img_haem, axis=0)\n",
    "    img_micro = np.expand_dims(img_micro, axis=0)\n",
    "\n",
    "    # Generate masks\n",
    "    \n",
    "    mask1 = model_hard_exudates.predict(img_hard, verbose=False)  # Predict mask\n",
    "    mask1 = (mask1 > 0.1).astype(int)  # Convert to binary\n",
    "    \n",
    "    mask2 = model_soft_exudates.predict(img_soft, verbose=False)  # Predict mask\n",
    "    mask2 = (mask2 > 0.1).astype(int)  # Convert to binary\n",
    "    \n",
    "    mask3 = model_haemorrhages.predict(img_haem, verbose=False)  # Predict mask\n",
    "    mask3 = (mask3 > 0.1).astype(int)  # Convert to binary\n",
    "    \n",
    "    mask4 = model_microaneurysms.predict(img_micro, verbose=False)  # Predict mask\n",
    "    mask4 = (mask4 > 0.1).astype(int)  # Convert to binary\n",
    "    \n",
    "    # make hard exudates mask red channel, soft exudates mask green channel, haemorrhages mask blue channel, microaneurysms mask alpha channel\n",
    "    mask1 = tf.image.resize(mask1, size)\n",
    "    mask2 = tf.image.resize(mask2, size)\n",
    "    mask3 = tf.image.resize(mask3, size)\n",
    "    mask4 = tf.image.resize(mask4, size)\n",
    "\n",
    "    # Combine masks into a single feature map\n",
    "    combined = np.concatenate([mask1, mask2, mask3, mask4])  # Shape: (H, W, 4)\n",
    "    combined = np.transpose(combined, (1, 2, 0, 3))\n",
    "    combined = np.squeeze(combined)\n",
    "\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "X_features = []\n",
    "y_labels = []\n",
    "\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing rows\"):\n",
    "    img = row['image']\n",
    "    label = row['label']\n",
    "    \n",
    "    image_path = os.path.join(dataset_path, img)\n",
    "    \n",
    "    combined_features = generate_feature_maps(image_path, size=(256, 256))\n",
    "    X_features.append(combined_features)\n",
    "    y_labels.append(label)\n",
    "\n",
    "X_features = np.array(X_features)\n",
    "y = np.array(y_labels)\n",
    "\n",
    "X_features.shape, y.shape\n",
    "\n",
    "y_present = (y > 0).astype(int)  # Binary: 0 (no disease), 1 (disease present)\n",
    "y_grades = np.where(y_present == 1, y, 0)  # Multiclass: 1-4 if disease present, 0 otherwise\n",
    "y_grades = to_categorical(y_grades, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_features)\n",
    "present_predictions = (predictions[0] > 0.5).astype(int)  # Binary output\n",
    "grade_predictions = np.argmax(predictions[1], axis=1)  # Multiclass output\n",
    "true_grades = np.argmax(y_grades, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(true_grades, grade_predictions)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(5), yticklabels=range(5))\n",
    "plt.xlabel(\"Predicted Grades\")\n",
    "plt.ylabel(\"True Grades\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_grades, grade_predictions)\n",
    "precision = precision_score(true_grades, grade_predictions, average='weighted')\n",
    "recall = recall_score(true_grades, grade_predictions, average='weighted')\n",
    "classification_rep = classification_report(true_grades, grade_predictions, target_names=[f'Grade {i}' for i in range(5)])\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted Cohen's kappa\n",
    "kappa = cohen_kappa_score(true_grades, grade_predictions, weights='linear')\n",
    "print(f\"Cohen's Kappa (weighted): {kappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted Cohen's kappa\n",
    "kappa = cohen_kappa_score(true_grades, grade_predictions, weights='quadratic')\n",
    "print(f\"Cohen's Kappa (weighted): {kappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict presence (binary)\n",
    "binary_predictions = (predictions[0] > 0.5).astype(int)  # Threshold at 0.5\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_present, binary_predictions)\n",
    "precision = precision_score(y_present, binary_predictions, average='binary')\n",
    "recall = recall_score(y_present, binary_predictions, average='binary')\n",
    "classification_rep = classification_report(y_present, binary_predictions, target_names=['No Disease', 'Disease'])\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDRiD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df_train = pd.read_csv('path/to/IDRiD/IDRiD_Disease Grading_Training Labels.csv')\n",
    "dataset_path_train = 'path/to/IDRiD/Disease Grading/Original Images/Training Set'\n",
    "\n",
    "df_test = pd.read_csv('path/to/IDRiD/Disease Grading/Groundtruths/IDRiD_Disease Grading_Testing Labels.csv')\n",
    "dataset_path_test = 'path/to/IDRiD/Disease Grading/Original Images/Testing Set'\n",
    "\n",
    "# Define constants\n",
    "IMAGE_SIZE = (256, 256)  # Example size, adjust to your models\n",
    "NUM_CLASSES = 5  # Grading levels 0-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet import preprocess_input as resenet50_preprocess_input\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models_TL/ResNet50_pretrained_enhanced.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights('models_TL/ResNet50_pretrained_enhanced.weights.h5')\n",
    "model.compile(optimizer='adam', loss=losses, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_img(img):\n",
    "    # Step 1: Apply median filter with a 3x3 kernel\n",
    "    img = cv2.medianBlur(img.astype(np.uint8), ksize=3)\n",
    "\n",
    "    # Step 2: Convert to LAB color space\n",
    "    lab_img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab_img)\n",
    "\n",
    "    # Step 3: Apply CLAHE on the Luminosity (L) channel with 8x8 tile grid\n",
    "    clahe = cv2.createCLAHE(clipLimit=6.0, tileGridSize=(8, 8))\n",
    "    l = clahe.apply(l)\n",
    "\n",
    "    # Step 4: Merge CLAHE enhanced L with original A and B channels\n",
    "    lab_img = cv2.merge((l, a, b))\n",
    "\n",
    "    # Step 5: Convert back to RGB color space\n",
    "    enhanced_img = cv2.cvtColor(lab_img, cv2.COLOR_LAB2RGB)\n",
    "    \n",
    "    return enhanced_img\n",
    "\n",
    "# Prepare dataset\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for index, row in tqdm(df_train.iterrows(), total=len(df_train), desc=\"Processing rows\"):\n",
    "    image_path = row['Image name'] + '.jpg'\n",
    "    label = row['Retinopathy grade']\n",
    "    \n",
    "    image_path = os.path.join(dataset_path_train, image_path)\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    x = img_to_array(img)\n",
    "    x = enhance_img(x)\n",
    "    x = resenet50_preprocess_input(x)\n",
    "    X.append(x)\n",
    "    y.append(label)\n",
    "    \n",
    "for index, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Processing rows\"):\n",
    "    image_path = row['Image name'] + '.jpg'\n",
    "    label = row['Retinopathy grade']\n",
    "    \n",
    "    image_path = os.path.join(dataset_path_test, image_path)\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    x = img_to_array(img)\n",
    "    x = enhance_img(x)\n",
    "    x = resenet50_preprocess_input(x)\n",
    "    X.append(x)\n",
    "    y.append(label)\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "y_present = (y > 0).astype(int)  # Binary: 0 (no disease), 1 (disease present)\n",
    "y_grades = np.where(y_present == 1, y, 0)  # Multiclass: 1-4 if disease present, 0 otherwise\n",
    "y_grades = to_categorical(y_grades, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X)\n",
    "present_predictions = (predictions[0] > 0.5).astype(int)  # Binary output\n",
    "grade_predictions = np.argmax(predictions[1], axis=1)  # Multiclass output\n",
    "true_grades = np.argmax(y_grades, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(true_grades, grade_predictions)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(5), yticklabels=range(5))\n",
    "plt.xlabel(\"Predicted Grades\")\n",
    "plt.ylabel(\"True Grades\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_grades, grade_predictions)\n",
    "precision = precision_score(true_grades, grade_predictions, average='weighted')\n",
    "recall = recall_score(true_grades, grade_predictions, average='weighted')\n",
    "classification_rep = classification_report(true_grades, grade_predictions, target_names=[f'Grade {i}' for i in range(5)])\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted Cohen's kappa\n",
    "kappa = cohen_kappa_score(true_grades, grade_predictions, weights='linear')\n",
    "print(f\"Cohen's Kappa (weighted): {kappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted Cohen's kappa\n",
    "kappa = cohen_kappa_score(true_grades, grade_predictions, weights='quadratic')\n",
    "print(f\"Cohen's Kappa (weighted): {kappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict presence (binary)\n",
    "binary_predictions = (predictions[0] > 0.5).astype(int)  # Threshold at 0.5\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_present, binary_predictions)\n",
    "precision = precision_score(y_present, binary_predictions, average='binary')\n",
    "recall = recall_score(y_present, binary_predictions, average='binary')\n",
    "classification_rep = classification_report(y_present, binary_predictions, target_names=['No Disease', 'Disease'])\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models_features/CNN_all_features.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights('models_features/CNN_all_features.weights.h5')\n",
    "model.compile(optimizer='adam', loss=losses, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EX\n",
    "with open('models_segmentation/EX.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "model_hard_exudates = model_from_json(model_json)\n",
    "model_hard_exudates.load_weights('models_segmentation/EX.weights.h5')\n",
    "model_hard_exudates.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# SE\n",
    "with open('models_segmentation/SE.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "model_soft_exudates = model_from_json(model_json)\n",
    "model_soft_exudates.load_weights('models_segmentation/SE.weights.h5')\n",
    "model_soft_exudates.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# HE\n",
    "with open('models_segmentation/HE.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "model_haemorrhages = model_from_json(model_json)\n",
    "model_haemorrhages.load_weights('models_segmentation/HE.weights.h5')\n",
    "model_haemorrhages.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# MA\n",
    "with open('models_segmentation/MA.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "model_microaneurysms = model_from_json(model_json)\n",
    "model_microaneurysms.load_weights('models_segmentation/MA.weights.h5')\n",
    "model_microaneurysms.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_img(img):\n",
    "    # Step 1: Apply median filter with a 3x3 kernel\n",
    "    img = cv2.medianBlur(img.astype(np.uint8), ksize=3)\n",
    "\n",
    "    # Step 2: Convert to LAB color space\n",
    "    lab_img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab_img)\n",
    "\n",
    "    # Step 3: Apply CLAHE on the Luminosity (L) channel with 8x8 tile grid\n",
    "    clahe = cv2.createCLAHE(clipLimit=6.0, tileGridSize=(8, 8))\n",
    "    l = clahe.apply(l)\n",
    "\n",
    "    # Step 4: Merge CLAHE enhanced L with original A and B channels\n",
    "    lab_img = cv2.merge((l, a, b))\n",
    "\n",
    "    # Step 5: Convert back to RGB color space\n",
    "    enhanced_img = cv2.cvtColor(lab_img, cv2.COLOR_LAB2RGB)\n",
    "    \n",
    "    return enhanced_img\n",
    "\n",
    "def get_mask(path, target_size):     \n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.resize(image, target_size)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert image to HSV (Hue, Saturation, Value) color space\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the red color range for masking\n",
    "    lower_red = np.array([0, 100, 50])\n",
    "    upper_red = np.array([12, 250, 250])\n",
    "    mask1 = cv2.inRange(hsv_image, lower_red, upper_red)\n",
    "\n",
    "    # lower_red2 = np.array([170, 120, 70])\n",
    "    lower_red2 = np.array([10, 60, 70])\n",
    "    upper_red2 = np.array([180, 255, 255])\n",
    "    mask2 = cv2.inRange(hsv_image, lower_red2, upper_red2)\n",
    "    mask2 = cv2.bitwise_not(mask2)\n",
    "\n",
    "    # Combine masks for red\n",
    "    red_mask = mask1 + mask2\n",
    "    \n",
    "    # overlay the mask on the original image\n",
    "    red_mask_3ch = cv2.cvtColor(red_mask, cv2.COLOR_GRAY2BGR)\n",
    "    mask = cv2.addWeighted(image_rgb, 0.7, red_mask_3ch, 0.3, 0)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Define function to preprocess images\n",
    "def preprocess_image_EX(image_path):\n",
    "    img = load_img(image_path, target_size=IMAGE_SIZE)\n",
    "    img = img_to_array(img)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def preprocess_image_SE(image_path):\n",
    "    img = load_img(image_path, target_size=IMAGE_SIZE)\n",
    "    img = img_to_array(img)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def preprocess_image_HE(image_path):\n",
    "    img = load_img(image_path, target_size=IMAGE_SIZE)\n",
    "    img = img_to_array(img)\n",
    "    img = enhance_img(img)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def preprocess_image_MA(image_path):\n",
    "    img = img = get_mask(image_path, target_size=IMAGE_SIZE)\n",
    "    img = img_to_array(img)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def generate_feature_maps(image_path, size=(128, 128)):\n",
    "    # Apply the specific preprocessing method for each model\n",
    "    img_hard = preprocess_image_EX(image_path)\n",
    "    img_soft = preprocess_image_SE(image_path)\n",
    "    img_haem = preprocess_image_HE(image_path)\n",
    "    img_micro = preprocess_image_MA(image_path)\n",
    "\n",
    "    # Add batch dimensions for predictions\n",
    "    img_hard = np.expand_dims(img_hard, axis=0)\n",
    "    img_soft = np.expand_dims(img_soft, axis=0)\n",
    "    img_haem = np.expand_dims(img_haem, axis=0)\n",
    "    img_micro = np.expand_dims(img_micro, axis=0)\n",
    "\n",
    "    # Generate masks\n",
    "    \n",
    "    mask1 = model_hard_exudates.predict(img_hard, verbose=False)  # Predict mask\n",
    "    mask1 = (mask1 > 0.1).astype(int)  # Convert to binary\n",
    "    \n",
    "    mask2 = model_soft_exudates.predict(img_soft, verbose=False)  # Predict mask\n",
    "    mask2 = (mask2 > 0.1).astype(int)  # Convert to binary\n",
    "    \n",
    "    mask3 = model_haemorrhages.predict(img_haem, verbose=False)  # Predict mask\n",
    "    mask3 = (mask3 > 0.1).astype(int)  # Convert to binary\n",
    "    \n",
    "    mask4 = model_microaneurysms.predict(img_micro, verbose=False)  # Predict mask\n",
    "    mask4 = (mask4 > 0.1).astype(int)  # Convert to binary\n",
    "    \n",
    "    # make hard exudates mask red channel, soft exudates mask green channel, haemorrhages mask blue channel, microaneurysms mask alpha channel\n",
    "    mask1 = tf.image.resize(mask1, size)\n",
    "    mask2 = tf.image.resize(mask2, size)\n",
    "    mask3 = tf.image.resize(mask3, size)\n",
    "    mask4 = tf.image.resize(mask4, size)\n",
    "\n",
    "    # Combine masks into a single feature map\n",
    "    combined = np.concatenate([mask1, mask2, mask3, mask4])  # Shape: (H, W, 4)\n",
    "    combined = np.transpose(combined, (1, 2, 0, 3))\n",
    "    combined = np.squeeze(combined)\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "X_features = []\n",
    "y_labels = []\n",
    "\n",
    "for index, row in tqdm(df_train.iterrows(), total=len(df_train), desc=\"Processing rows\"):\n",
    "    image_path = row['Image name'] + '.jpg'\n",
    "    label = row['Retinopathy grade']\n",
    "    \n",
    "    image_path = os.path.join(dataset_path_train, image_path)\n",
    "    combined_features = generate_feature_maps(image_path, size=(256, 256))\n",
    "    X_features.append(combined_features)\n",
    "    y_labels.append(label)\n",
    "\n",
    "\n",
    "for index, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Processing rows\"):\n",
    "    image_path = row['Image name'] + '.jpg'\n",
    "    label = row['Retinopathy grade']\n",
    "    \n",
    "    image_path = os.path.join(dataset_path_train, image_path)\n",
    "    combined_features = generate_feature_maps(image_path, size=(256, 256))\n",
    "    X_features.append(combined_features)\n",
    "    y_labels.append(label)\n",
    "\n",
    "X_features = np.array(X_features)\n",
    "y_labels = np.array(y_labels)\n",
    "\n",
    "y_present = (y_labels > 0).astype(int)  # Binary: 0 (no disease), 1 (disease present)\n",
    "y_grades = np.where(y_present == 1, y_labels, 0)  # Multiclass: 1-4 if disease present, 0 otherwise\n",
    "y_grades = to_categorical(y_grades, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_features)\n",
    "present_predictions = (predictions[0] > 0.5).astype(int)  # Binary output\n",
    "grade_predictions = np.argmax(predictions[1], axis=1)  # Multiclass output\n",
    "true_grades = np.argmax(y_grades, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(true_grades, grade_predictions)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(5), yticklabels=range(5))\n",
    "plt.xlabel(\"Predicted Grades\")\n",
    "plt.ylabel(\"True Grades\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_grades, grade_predictions)\n",
    "precision = precision_score(true_grades, grade_predictions, average='weighted')\n",
    "recall = recall_score(true_grades, grade_predictions, average='weighted')\n",
    "classification_rep = classification_report(true_grades, grade_predictions, target_names=[f'Grade {i}' for i in range(5)])\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted Cohen's kappa\n",
    "kappa = cohen_kappa_score(true_grades, grade_predictions, weights='linear')\n",
    "print(f\"Cohen's Kappa (weighted): {kappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted Cohen's kappa\n",
    "kappa = cohen_kappa_score(true_grades, grade_predictions, weights='quadratic')\n",
    "print(f\"Cohen's Kappa (weighted): {kappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict presence (binary)\n",
    "binary_predictions = (predictions[0] > 0.5).astype(int)  # Threshold at 0.5\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_present, binary_predictions)\n",
    "precision = precision_score(y_present, binary_predictions, average='binary')\n",
    "recall = recall_score(y_present, binary_predictions, average='binary')\n",
    "classification_rep = classification_report(y_present, binary_predictions, target_names=['No Disease', 'Disease'])\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
